{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hosted by Developer Student Club of the University of Florida\n",
    "#### Tech Leads: Tyler Metzger, Raymond Yu, Sindhu Kandula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will go over some common techniques involving data cleaning. Specifically, we will discuss how to handle missing or faulty entries and apply scaling or normalization on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Read in datasets\n",
    "nfl_data = pd.read_csv(\"./NFL Play by Play 2009-2016 (v3).csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looks at first 5 rows of dataset\n",
    "nfl_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get number of missing values for each column\n",
    "missing_values_count = nfl_data.isnull().sum()\n",
    "#Counts for first 10 columns\n",
    "missing_values_count[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calulate percent of data that is missing\n",
    "total_cells = np.product(nfl_data.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "percent_missing = (total_missing/total_cells) * 100\n",
    "print(\"%f%% of data is null\" % (percent_missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns with at least one missing value\n",
    "columns_with_na_dropped = nfl_data.dropna(axis=1)\n",
    "columns_with_na_dropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loss\n",
    "print(\"Columns in original dataset: %d \\n\" % nfl_data.shape[1])\n",
    "print(\"Columns without null values: %d\" % columns_with_na_dropped.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a subset\n",
    "subset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head()\n",
    "subset_nfl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace null values with 0\n",
    "subset_nfl_data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Box-Cox Transformation\n",
    "from scipy import stats\n",
    "\n",
    "# plotting modules\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates 1000 data points randomly from an exponential distribution\n",
    "original_data = np.random.exponential(size=1000)\n",
    "# print(original_data)\n",
    "\n",
    "# scale data between 0 and 1\n",
    "scaled_data = np.interp(original_data, (original_data.min(), original_data.max()), (0, +1))\n",
    "\n",
    "# plot both graphs\n",
    "fig, ax = plt.subplots(1,2)\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(scaled_data, ax=ax[1])\n",
    "ax[1].set_title(\"Scaled data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the exponential data with boxcox\n",
    "normalized_data = stats.boxcox(original_data)\n",
    "\n",
    "# plot both graphs\n",
    "fig, ax=plt.subplots(1,2)\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(normalized_data[0], ax=ax[1])\n",
    "ax[1].set_title(\"Normalized data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
